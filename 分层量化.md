# 1ã€å‡†å¤‡é˜¶æ®µ
## åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
1. ä½¿ç”¨ä¸‹é¢çš„è¯­å¥åœ¨å½“å‰æ–‡ä»¶å¤¹åˆ›å»ºè™šæ‹Ÿç¯å¢ƒæ–‡ä»¶å¤¹`myenv`
```bash
conda create -p ./myenv python=3.10.0 -y
```
2. æ¿€æ´»bashï¼Œè™½ç„¶å·²ç»åˆ›å»ºäº†è™šæ‹Ÿç¯å¢ƒï¼Œä½†æ˜¯shellè¿˜æ²¡æœ‰è¯†åˆ«
```bash
conda init bash
source ~/.bashrc
```
3. æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
```bash
conda activate ./myenv
```
4. ä¸‹è½½ä¾èµ–
```bash
conda install pip
pip install vllm
pip install llmcompressor
```

## è§£å†³ç½‘ç»œé—®é¢˜
åœ¨AutoDLçš„å¸®åŠ©æ–‡æ¡£ä¸­æ‰¾åˆ°â€œå­¦æœ¯èµ„æºåŠ é€Ÿâ€  
å‘½ä»¤è¡Œè¾“å…¥æŒ‡ä»¤ï¼š
```
source /etc/network_turbo
```
**å¦‚æœè¦ç”¨åˆ°huggingfaceçš„æ•°æ®é›†**ï¼Œè¿˜éœ€è¦ï¼š
```
export HF_ENDPOINT=https://hf-mirror.com
```

# 2ã€æ¨¡å‹ä¸‹è½½
ä½¿ç”¨é­”æ­å¹³å°ä¸€é”®ä¸‹è½½(https://www.modelscope.cn/)
```bash
modelscope download --model Qwen/Qwen2.5-7B-Instruct --local_dir ./models/Qwen2.5-7B-Instruct
```

# 3ã€æŸ¥çœ‹æ¨¡å‹ç»“æ„
```python
"""
browser.py
æŒ‡å®šæœ¬åœ°æ¨¡å‹è·¯å¾„ï¼Œæµè§ˆå¹¶å¯¼å‡ºæ‰€æœ‰å­æ¨¡å—åç§°ä¸ç±»å‹
"""
from pathlib import Path
from transformers import AutoConfig, AutoModelForCausalLM
import torch

# ---------------- å”¯ä¸€éœ€è¦æ”¹çš„å˜é‡ ----------------
MODEL_PATH = Path(r"./models/Qwen2.5-7B-Instruct")  # <-- ä½ çš„æœ¬åœ°ç›®å½•
# --------------------------------------------------

def build_model(model_path, device_map="cpu"):
    """ä»…åŠ è½½æ¨¡å‹ç»“æ„ï¼Œæƒé‡ mmap åŠ è½½ï¼Œä¸å æ˜¾å­˜"""
    return AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype="auto",
        device_map=device_map,
        local_files_only=True,
        low_cpu_mem_usage=True
    )

def show_and_save(model, max_depth=4, save_file="model_layers.txt"):
    """é€’å½’æ‰“å° + å†™å…¥æ–‡ä»¶"""
    fout = open(save_file, "w", encoding="utf-8")

    def _walk(module, prefix="", depth=0):
        for name, child in module.named_children():
            full = prefix + name
            cls = child.__class__.__name__
            line = f"{' '*depth}{full:<60s} {cls}"
            print(line)
            fout.write(line + "\n")
            if depth < max_depth:
                _walk(child, full + ".", depth + 1)

    _walk(model)
    fout.close()
    print(f"\nğŸ‘‰ ç»“æœå·²å†™å…¥ {save_file.resolve()}")

if __name__ == "__main__":
    assert MODEL_PATH.exists(), f"è·¯å¾„ä¸å­˜åœ¨ï¼š{MODEL_PATH}"
    print(">>> åŠ è½½æ¨¡å‹ç»“æ„ â€¦")
    model = build_model(MODEL_PATH)
    print(">>> ç”Ÿæˆå±‚åˆ—è¡¨ â€¦\n")
    show_and_save(model)
```


# 4ã€æ¨¡å‹é‡åŒ–
```python
import os
import subprocess
from pathlib import Path

# ---------------- 1. å‚æ•° ----------------
MODEL_ID = "Qwen2.5-7B-Instruct"
LOCAL_MODEL_DIR = Path("./models") / MODEL_ID.split("/")[-1]   # ./models
SAVE_QUANT_DIR = LOCAL_MODEL_DIR.parent / f"{LOCAL_MODEL_DIR.name}-W8A8"

NUM_CALIBRATION_SAMPLES = 512
MAX_SEQUENCE_LENGTH = 2048

# ---------------- 2. è®¾ç½®é•œåƒï¼ˆæ¸…åï¼‰ ----------------
# åªå¯¹æœ¬æ¬¡è¿è¡Œç”Ÿæ•ˆï¼Œä¸æ±¡æŸ“å…¨å±€
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

# ---------------- 4. åŠ è½½æœ¬åœ°æ¨¡å‹/åˆ†è¯å™¨ ----------------
from transformers import AutoTokenizer, AutoModelForCausalLM

print(">>> ä»æœ¬åœ°åŠ è½½æ¨¡å‹ â€¦")
model = AutoModelForCausalLM.from_pretrained(
    LOCAL_MODEL_DIR,
    dtype="auto",
    device_map="auto",
    local_files_only=True
)
tokenizer = AutoTokenizer.from_pretrained(
    LOCAL_MODEL_DIR,
    local_files_only=True,
    use_fast=True
)

# ---------------- 5. å‡†å¤‡æ ¡å‡†æ•°æ® ----------------
from datasets import load_dataset

print(">>> åŠ è½½æ ¡å‡†æ•°æ®é›† â€¦")
ds = load_dataset(
    "HuggingFaceH4/ultrachat_200k",
    split=f"train_sft[:{NUM_CALIBRATION_SAMPLES}]"
)
ds = ds.shuffle(seed=42)

def preprocess(example):
    return {"text": tokenizer.apply_chat_template(example["messages"], tokenize=False)}

ds = ds.map(preprocess)

def tokenize(sample):
    return tokenizer(sample["text"],
                     padding=False,
                     max_length=MAX_SEQUENCE_LENGTH,
                     truncation=True,
                     add_special_tokens=False)

ds = ds.map(tokenize, remove_columns=ds.column_names)

# ---------------- 6. é‡åŒ– ----------------
from llmcompressor import oneshot
from llmcompressor.modifiers.quantization import GPTQModifier

recipe = GPTQModifier(targets="Linear", scheme="W8A8", ignore=["lm_head"])

print(">>> å¼€å§‹ GPTQ é‡åŒ– â€¦")
oneshot(
    model=model,
    dataset=ds,
    recipe=recipe,
    max_seq_length=MAX_SEQUENCE_LENGTH,
    num_calibration_samples=NUM_CALIBRATION_SAMPLES,
)

# ---------------- 7. ä¿å­˜é‡åŒ–ç»“æœ ----------------
print(">>> ä¿å­˜é‡åŒ–æ¨¡å‹ â€¦")
SAVE_QUANT_DIR.mkdir(parents=True, exist_ok=True)
model.save_pretrained(SAVE_QUANT_DIR, save_compressed=True)
tokenizer.save_pretrained(SAVE_QUANT_DIR)
print(">>> é‡åŒ–å®Œæˆï¼Œå·²ä¿å­˜åˆ°ï¼š", SAVE_QUANT_DIR)
```

# 5ã€æ¨¡å‹è¯„ä¼°
*è®°å¾—å…ˆæ‰“å¼€æ¢¯å­å†è¿›è¡Œè¯„ä¼°ï¼Œå¦åˆ™è¯„ä¼°æ—¶çš„æ•°æ®é›†æ— æ³•æˆåŠŸåŠ è½½*
å…ˆè¿è¡Œï¼š
```python
from vllm import LLM
model = LLM("./models/Llama", gpu_memory_utilization=0.95) # é˜²æ­¢çˆ†æ˜¾å­˜!
```
ç„¶åå‘½ä»¤è¡Œï¼š
```bash
lm_eval --model vllm \
  --model_args pretrained="./models/TinyLlama-1.1B-Chat-v1.0-W8A8",add_bos_token=true \
  --tasks gsm8k \
  --num_fewshot 5 \
  --limit 250 \
  --batch_size 'auto' \
  --output_path eval_out/Llama_out
```

## å¦‚æœå‡ºç°å‘½ä»¤è¡Œè¯´æ— æ³•è¯†åˆ«lm_evalé—®é¢˜
ç›´æ¥ä¸‹è½½
```bash
pip install lm_eval
```
