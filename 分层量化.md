# 1ã€å‡†å¤‡é˜¶æ®µ
## åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
1. ä½¿ç”¨ä¸‹é¢çš„è¯­å¥åœ¨å½“å‰æ–‡ä»¶å¤¹åˆ›å»ºè™šæ‹Ÿç¯å¢ƒæ–‡ä»¶å¤¹`myenv`
```bash
conda create -p ./myenv python=3.10.0 -y
```
2. æ¿€æ´»bashï¼Œè™½ç„¶å·²ç»åˆ›å»ºäº†è™šæ‹Ÿç¯å¢ƒï¼Œä½†æ˜¯shellè¿˜æ²¡æœ‰è¯†åˆ«
```bash
conda init bash
source ~/.bashrc
```
3. æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
```bash
conda activate ./myenv
```
4. ä¸‹è½½ä¾èµ–
```bash
conda install pip
pip install vllm
pip install llmcompressor
```

## è§£å†³ç½‘ç»œé—®é¢˜
åœ¨AutoDLçš„å¸®åŠ©æ–‡æ¡£ä¸­æ‰¾åˆ°â€œå­¦æœ¯èµ„æºåŠ é€Ÿâ€  
å‘½ä»¤è¡Œè¾“å…¥æŒ‡ä»¤ï¼š
```
source /etc/network_turbo
```
**å¦‚æœè¦ç”¨åˆ°huggingfaceçš„æ•°æ®é›†**ï¼Œè¿˜éœ€è¦ï¼š
```
export HF_ENDPOINT=https://hf-mirror.com
```

# 2ã€æ¨¡å‹å’Œæ•°æ®é›†ä¸‹è½½
ä½¿ç”¨é­”æ­å¹³å°ä¸€é”®ä¸‹è½½(https://www.modelscope.cn/)
```bash
modelscope download --model Qwen/Qwen2.5-7B-Instruct --local_dir ./models/Qwen2.5-7B-Instruct
```

æ•°æ®é›†ä½¿ç”¨huggingfaceä¸Šçš„å…¬å¼€æ•°æ®é›†
```python
from datasets import load_dataset

# è®¾ç½®æœ¬åœ°ç¼“å­˜è·¯å¾„
custom_cache_dir = "./datasets/mmlu"
ds = load_dataset("cais/mmlu", "all", cache_dir=custom_cache_dir)
```

ä¸‹è½½`mmlu`æ—¶å‡ºç°çš„ä¸€äº›é—®é¢˜ï¼š
1. READMEæç¤ºä¸‹è½½æ•°é‡ä¸åŒé—®é¢˜ï¼šå–æ¶ˆé•œåƒç«™ï¼Œç›´æ¥ç”¨å®˜æ–¹ç½‘ç«™ä¸‹è½½
2. CASè¿æ¥é—®é¢˜ï¼šå¯ç”¨é•œåƒç«™

# 3ã€æŸ¥çœ‹æ¨¡å‹ç»“æ„
```python
"""
browser.py
æŒ‡å®šæœ¬åœ°æ¨¡å‹è·¯å¾„ï¼Œæµè§ˆå¹¶å¯¼å‡ºæ‰€æœ‰å­æ¨¡å—åç§°ä¸ç±»å‹
"""
from pathlib import Path
from transformers import AutoConfig, AutoModelForCausalLM
import torch

# ---------------- å”¯ä¸€éœ€è¦æ”¹çš„å˜é‡ ----------------
MODEL_PATH = Path(r"./models/Qwen2.5-7B-Instruct")  # <-- ä½ çš„æœ¬åœ°ç›®å½•
# --------------------------------------------------

def build_model(model_path, device_map="cpu"):
    """ä»…åŠ è½½æ¨¡å‹ç»“æ„ï¼Œæƒé‡ mmap åŠ è½½ï¼Œä¸å æ˜¾å­˜"""
    return AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype="auto",
        device_map=device_map,
        local_files_only=True,
        low_cpu_mem_usage=True
    )

def show_and_save(model, max_depth=4, save_file="model_layers.txt"):
    """é€’å½’æ‰“å° + å†™å…¥æ–‡ä»¶"""
    fout = open(save_file, "w", encoding="utf-8")

    def _walk(module, prefix="", depth=0):
        for name, child in module.named_children():
            full = prefix + name
            cls = child.__class__.__name__
            line = f"{' '*depth}{full:<60s} {cls}"
            print(line)
            fout.write(line + "\n")
            if depth < max_depth:
                _walk(child, full + ".", depth + 1)

    _walk(model)
    fout.close()
    print(f"\nğŸ‘‰ ç»“æœå·²å†™å…¥ {save_file.resolve()}")

if __name__ == "__main__":
    assert MODEL_PATH.exists(), f"è·¯å¾„ä¸å­˜åœ¨ï¼š{MODEL_PATH}"
    print(">>> åŠ è½½æ¨¡å‹ç»“æ„ â€¦")
    model = build_model(MODEL_PATH)
    print(">>> ç”Ÿæˆå±‚åˆ—è¡¨ â€¦\n")
    show_and_save(model)
```


# 4ã€æ¨¡å‹é‡åŒ–
## å¯¹è¯æ•°æ®é›†é‡åŒ–ç¤ºä¾‹
```python
import os
import subprocess
from pathlib import Path

# ---------------- 1. å‚æ•° ----------------
MODEL_ID = "Qwen2.5-7B-Instruct"
LOCAL_MODEL_DIR = Path("./models") / MODEL_ID.split("/")[-1]   # ./models
SAVE_QUANT_DIR = LOCAL_MODEL_DIR.parent / f"{LOCAL_MODEL_DIR.name}-W8A8"

NUM_CALIBRATION_SAMPLES = 512
MAX_SEQUENCE_LENGTH = 2048

# ---------------- 2. è®¾ç½®é•œåƒï¼ˆæ¸…åï¼‰ ----------------
# åªå¯¹æœ¬æ¬¡è¿è¡Œç”Ÿæ•ˆï¼Œä¸æ±¡æŸ“å…¨å±€
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

# ---------------- 3. åŠ è½½æœ¬åœ°æ¨¡å‹/åˆ†è¯å™¨ ----------------
from transformers import AutoTokenizer, AutoModelForCausalLM

print(">>> ä»æœ¬åœ°åŠ è½½æ¨¡å‹ â€¦")
model = AutoModelForCausalLM.from_pretrained(
    LOCAL_MODEL_DIR,
    dtype="auto",
    device_map="auto",
    local_files_only=True
)
tokenizer = AutoTokenizer.from_pretrained(
    LOCAL_MODEL_DIR,
    local_files_only=True,
    use_fast=True
)

# ---------------- 4. å‡†å¤‡æ ¡å‡†æ•°æ® ----------------
from datasets import load_dataset

print(">>> åŠ è½½æ ¡å‡†æ•°æ®é›† â€¦")
ds = load_dataset(
    "HuggingFaceH4/ultrachat_200k",
    split=f"train_sft[:{NUM_CALIBRATION_SAMPLES}]"
)
ds = ds.shuffle(seed=42)

def preprocess(example):
    return {"text": tokenizer.apply_chat_template(example["messages"], tokenize=False)}

ds = ds.map(preprocess)

def tokenize(sample):
    return tokenizer(sample["text"],
                     padding=False,
                     max_length=MAX_SEQUENCE_LENGTH,
                     truncation=True,
                     add_special_tokens=False)

ds = ds.map(tokenize, remove_columns=ds.column_names)

# ---------------- 5. é‡åŒ– ----------------
from llmcompressor import oneshot
from llmcompressor.modifiers.quantization import GPTQModifier

recipe = GPTQModifier(targets="Linear", scheme="W8A8", ignore=["lm_head"])

print(">>> å¼€å§‹ GPTQ é‡åŒ– â€¦")
oneshot(
    model=model,
    dataset=ds,
    recipe=recipe,
    max_seq_length=MAX_SEQUENCE_LENGTH,
    num_calibration_samples=NUM_CALIBRATION_SAMPLES,
)

# ---------------- 6. ä¿å­˜é‡åŒ–ç»“æœ ----------------
print(">>> ä¿å­˜é‡åŒ–æ¨¡å‹ â€¦")
SAVE_QUANT_DIR.mkdir(parents=True, exist_ok=True)
model.save_pretrained(SAVE_QUANT_DIR, save_compressed=True)
tokenizer.save_pretrained(SAVE_QUANT_DIR)
print(">>> é‡åŒ–å®Œæˆï¼Œå·²ä¿å­˜åˆ°ï¼š", SAVE_QUANT_DIR)
```

**æ³¨æ„ä¸åŒçš„æ•°æ®é›†çš„å‡†å¤‡è¿‡ç¨‹ä¸åŒ**
## mmluéƒ¨åˆ†å­¦ç§‘ç¤ºä¾‹
```python
# ---------------- 5. å‡†å¤‡æ ¡å‡†æ•°æ® ----------------
print(">>> åŠ è½½æ ¡å‡†æ•°æ®é›† â€¦")
from datasets import load_dataset, concatenate_datasets

subjects = ["abstract_algebra",
            "high_school_mathematics", 
            "high_school_chemistry", 
            "high_school_computer_science",
            "high_school_physics",
            "machine_learning",
            "high_school_biology"
]

# é€ä¸ªåŠ è½½
ds_dict = {
    s: load_dataset("cais/mmlu", s)   # æ³¨æ„è¿™é‡Œç¬¬äºŒä¸ªå‚æ•°æ˜¯å­—ç¬¦ä¸²
    for s in subjects
}

# å¦‚æœä½ æƒ³æŠŠæ‰€æœ‰å­¦ç§‘åˆå¹¶æˆä¸€ä¸ªå¤§ Dataset
ds = concatenate_datasets([
    ds_dict[s]["test"]     # MMLU é»˜è®¤ split å« test/dev/val
    for s in subjects
])

import random

SYSTEM_PROMPT = (
    "Answer the following multiple-choice question. "
    "Only output the letter of the correct choice, nothing else."
)

def mmlu_to_text(example):
    """
    æŠŠ MMLU åŸå§‹å­—æ®µ -> ä¸€æ®µçº¯æ–‡æœ¬
    """
    q = example["question"].strip()
    choices = example["choices"]          # å·²æ˜¯ä¸ª list[str]
    answer_idx = example["answer"]        # 0/1/2/3
    label = "ABCD"[answer_idx]

    # æ‹¼æˆä¸€æ®µè¿ç»­æ–‡æœ¬
    choice_str = " ".join(choices)        # ä¾‹ï¼šA) xxx B) yyy C) zzz D) www
    text = f"{SYSTEM_PROMPT}\nQuestion: {q}\n{choice_str}\nAnswer: {label}"
    return {"text": text}

# 1. ç”Ÿæˆçº¯æ–‡æœ¬
ds = ds.map(mmlu_to_text)

# 2. ç›´æ¥ tokenizeï¼ˆä¸æ·»åŠ  special tokensï¼Œè®©åºåˆ—å°½é‡é•¿ï¼‰
def tokenize(sample):
    return tokenizer(
        sample["text"],
        padding=False,
        truncation=True,
        max_length=MAX_SEQUENCE_LENGTH,
        add_special_tokens=False   # å·²åŒ…å«åœ¨æ¨¡æ¿é‡Œå°±åˆ«å†åŠ 
    )

ds = ds.map(tokenize, remove_columns=ds.column_names)

# 3. è¿‡æ»¤æ‰å¤ªçŸ­çš„æ ·æœ¬ï¼ˆå¯é€‰ï¼Œé˜²æ­¢ pad å¤ªå¤šï¼‰
ds = ds.filter(lambda x: len(x["input_ids"]) >= 64)

# 4. æ‰“ä¹± & å–å‰ N æ¡åšæ ¡å‡†
ds = ds.shuffle(seed=42).select(range(min(NUM_CALIBRATION_SAMPLES, len(ds))))
```

## mmlué€‰æ‹©allç¤ºä¾‹
```python
print(">>> åŠ è½½æ ¡å‡†æ•°æ®é›† â€¦")
custom_cache_dir = "./datasets/mmlu"
ds_dict = load_dataset("cais/mmlu", "all", cache_dir=custom_cache_dir)

# âœ… è‡ªåŠ¨é€‰æ‹©åˆé€‚ splitï¼ˆä¼˜å…ˆ test / validationï¼‰
preferred_splits = ["test", "validation", "dev", "auxiliary_train"]

ds = ds_dict["validation"]

SYSTEM_PROMPT = (
    "Answer the following multiple-choice question. "
    "Only output the letter of the correct choice, nothing else."
)

def mmlu_to_text(example):
    q = example["question"].strip()
    choices = example["choices"]
    answer_idx = example["answer"]
    label = "ABCD"[answer_idx]
    choice_str = " ".join([f"{c}) {text}" for c, text in zip("ABCD", choices)])
    text = f"{SYSTEM_PROMPT}\nQuestion: {q}\n{choice_str}\nAnswer: {label}"
    return {"text": text}

# 1. è½¬æ¢ä¸ºæ–‡æœ¬
print(">>> æ ¼å¼åŒ–æ•°æ® â€¦")
ds = ds.map(mmlu_to_text)

# 2. tokenizeï¼ˆä¸åŠ  special tokensï¼‰
def tokenize(sample):
    return tokenizer(
        sample["text"],
        padding=False,
        truncation=True,
        max_length=MAX_SEQUENCE_LENGTH,
        add_special_tokens=False
    )

ds = ds.map(tokenize, remove_columns=ds.column_names)

# 3. è¿‡æ»¤ & æ‰“ä¹±
ds = ds.filter(lambda x: len(x["input_ids"]) >= 64)
ds = ds.shuffle(seed=42).select(range(min(NUM_CALIBRATION_SAMPLES, len(ds))))
print(f">>> æ ¡å‡†æ ·æœ¬æ•°: {len(ds)}")
```

# 5ã€æ¨¡å‹è¯„ä¼°
*è®°å¾—å…ˆæ‰“å¼€æ¢¯å­å†è¿›è¡Œè¯„ä¼°ï¼Œå¦åˆ™è¯„ä¼°æ—¶çš„æ•°æ®é›†æ— æ³•æˆåŠŸåŠ è½½*
å…ˆè¿è¡Œï¼š
```python
from vllm import LLM
model = LLM("./models/Llama", gpu_memory_utilization=0.95) # é˜²æ­¢çˆ†æ˜¾å­˜!
```
ç„¶åå‘½ä»¤è¡Œï¼š
```bash
lm_eval --model vllm \
  --model_args pretrained="./models/TinyLlama-1.1B-Chat-v1.0-W8A8",add_bos_token=true \
  --tasks gsm8k \
  --num_fewshot 5 \
  --limit 250 \
  --batch_size 'auto' \
  --output_path eval_out/Llama_out
```

taskå¯é€‰ä»»åŠ¡
| ç±»å‹            | ç¤ºä¾‹ä»»åŠ¡åç§° (`--tasks`)                                                     | è¯´æ˜            |
| ------------- | ---------------------------------------------------------------------- | ------------- |
| è¯­è¨€ç†è§£ / é˜…è¯»ç†è§£   | `hellaswag`, `piqa`, `winogrande`, `arc_easy`, `arc_challenge`, `mmlu` | å•é€‰é¢˜ã€å¤šé€‰é¢˜ã€å¸¸è¯†æ¨ç†ç­‰ |
| æ•°å­¦            | `gsm8k`, `mathqa`, `aqua_rat`                                          | æ•°å­¦é¢˜è¯„æµ‹         |
| ç¿»è¯‘            | `flores_200`                                                           | å¤šè¯­è¨€ç¿»è¯‘         |
| QA / å¯¹è¯       | `triviaqa`, `nq_open`                                                  | å¼€æ”¾å¼é—®ç­”         |
| ä¸­æ–‡ä»»åŠ¡ï¼ˆè‹¥ä½ çš„ç‰ˆæœ¬æ”¯æŒï¼‰ | `ceval`, `cmmlu`                                                       | ä¸­æ–‡èƒ½åŠ›è¯„æµ‹        |
| å¤šä»»åŠ¡é›†åˆ         | `mmlu`, `mmlu_pro`                                                     | æ¶µç›–å¤šä¸ªå­¦ç§‘        |


## å¦‚æœå‡ºç°å‘½ä»¤è¡Œè¯´æ— æ³•è¯†åˆ«lm_evalé—®é¢˜
ç›´æ¥ä¸‹è½½
```bash
pip install lm_eval
```
