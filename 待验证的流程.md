# 1、准备阶段
## 创建虚拟环境
1. 使用下面的语句在当前文件夹创建虚拟环境文件夹`myenv`
```bash
conda create -p ./myenv
```
2. 激活bash，虽然已经创建了虚拟环境，但是shell还没有识别
```bash
conda init bash
source ~/.bashrc
```
3. 激活虚拟环境
```bash
conda activate ./myenv
```
4. 下载依赖
```bash
conda install pip
pip install llmcompressor
```

## 解决网络问题
在AutoDL的帮助文档中找到“学术资源加速”  
命令行输入指令：
```
source /etc/network_turbo
```

# 2、模型下载
```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
本地下载 + 量化 Meta-Llama-3-8B-Instruct
W4A16-G128，使用 HuggingFace 镜像加速
"""
import os
import subprocess
from pathlib import Path

# ---------------- 1. 参数 ----------------
MODEL_ID = "meta-llama/Meta-Llama-3-8B-Instruct"
LOCAL_MODEL_DIR = Path("./models") / MODEL_ID.split("/")[-1]   # ./models/Meta-Llama-3-8B-Instruct
SAVE_QUANT_DIR = LOCAL_MODEL_DIR.parent / f"{LOCAL_MODEL_DIR.name}-W4A16-G128"

NUM_CALIBRATION_SAMPLES = 512
MAX_SEQUENCE_LENGTH = 2048

# ---------------- 2. 设置镜像（清华） ----------------
# 只对本次运行生效，不污染全局
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

# ---------------- 3. 下载模型到本地 ----------------
def download_model(repo_id: str, local_dir: Path):
    """用 huggingface-cli 下载完整模型，支持断点续传"""
    local_dir.mkdir(parents=True, exist_ok=True)
    cmd = [
        "huggingface-cli", "download",
        repo_id,
        "--local-dir", str(local_dir),
        "--local-dir-use-symlinks", "False",   # 真正落盘，不软链
        "--resume-download"                    # 断点续传
    ]
    print(">>> 开始下载模型，可能耗时较长，请耐心等待 …")
    subprocess.check_call(cmd)
    print(">>> 模型已下载到：", local_dir)

if not (LOCAL_MODEL_DIR / "config.json").exists():
    download_model(MODEL_ID, LOCAL_MODEL_DIR)
else:
    print(">>> 本地模型已存在，跳过下载")
```

# 3、模型量化
```python
# ---------------- 4. 加载本地模型/分词器 ----------------
from transformers import AutoTokenizer, AutoModelForCausalLM, torch_dtype

print(">>> 从本地加载模型 …")
model = AutoModelForCausalLM.from_pretrained(
    LOCAL_MODEL_DIR,
    torch_dtype="auto",
    device_map="auto",
    local_files_only=True
)
tokenizer = AutoTokenizer.from_pretrained(
    LOCAL_MODEL_DIR,
    local_files_only=True,
    use_fast=True
)

# ---------------- 5. 准备校准数据 ----------------
from datasets import load_dataset

print(">>> 加载校准数据集 …")
ds = load_dataset("HuggingFaceH4/ultrachat_200k",
                  split=f"train_sft[:{NUM_CALIBRATION_SAMPLES}]",
                  trust_remote_code=True)
ds = ds.shuffle(seed=42)

def preprocess(example):
    return {"text": tokenizer.apply_chat_template(example["messages"], tokenize=False)}

ds = ds.map(preprocess)

def tokenize(sample):
    return tokenizer(sample["text"],
                     padding=False,
                     max_length=MAX_SEQUENCE_LENGTH,
                     truncation=True,
                     add_special_tokens=False)

ds = ds.map(tokenize, remove_columns=ds.column_names)

# ---------------- 6. 量化 ----------------
from llmcompressor import oneshot
from llmcompressor.modifiers.quantization import GPTQModifier

recipe = GPTQModifier(targets="Linear", scheme="W4A16", ignore=["lm_head"])

print(">>> 开始 GPTQ 量化 …")
oneshot(
    model=model,
    dataset=ds,
    recipe=recipe,
    max_seq_length=MAX_SEQUENCE_LENGTH,
    num_calibration_samples=NUM_CALIBRATION_SAMPLES,
)

# ---------------- 7. 保存量化结果 ----------------
print(">>> 保存量化模型 …")
SAVE_QUANT_DIR.mkdir(parents=True, exist_ok=True)
model.save_pretrained(SAVE_QUANT_DIR, save_compressed=True)
tokenizer.save_pretrained(SAVE_QUANT_DIR)
print(">>> 量化完成，已保存到：", SAVE_QUANT_DIR)
```

# 4、模型评估
先运行：
```python
from vllm import LLM
model = LLM("./models/TinyLlama-1.1B-Chat-v1.0-W4A16")
```
然后命令行：
```bash
lm_eval --model vllm \
  --model_args pretrained="./models/TinyLlama-1.1B-Chat-v1.0-W4A16",add_bos_token=true \
  --tasks gsm8k \
  --num_fewshot 5 \
  --limit 250 \
  --batch_size 'auto' \
  --output_path eval_out/Llama_out
```
