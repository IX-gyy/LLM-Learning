# 1、准备阶段
## 创建虚拟环境
1. 使用下面的语句在当前文件夹创建虚拟环境文件夹`myenv`
```bash
conda create -p ./myenv python=3.10.0 -y
```
2. 激活bash，虽然已经创建了虚拟环境，但是shell还没有识别
```bash
conda init bash
source ~/.bashrc
```
3. 激活虚拟环境
```bash
conda activate ./myenv
```
4. 下载依赖
```bash
conda install pip
pip install vllm
pip install llmcompressor
```

## 解决网络问题
在AutoDL的帮助文档中找到“学术资源加速”  
命令行输入指令：
```
source /etc/network_turbo
```
**如果要用到huggingface的数据集**，还需要：
```
export HF_ENDPOINT=https://hf-mirror.com
```

# 2、模型下载
使用魔搭平台一键下载(https://www.modelscope.cn/)
```bash
modelscope download --model AI-ModelScope/TinyLlama-1.1B-Chat-v1.0 --local_dir ./models/TinyLlama-1.1B-Chat-v1.0
```


# 3、模型量化
```python
import os
import subprocess
from pathlib import Path

# ---------------- 1. 参数 ----------------
MODEL_ID = "TinyLlama-1.1B-Chat-v1.0"
LOCAL_MODEL_DIR = Path("./models") / MODEL_ID.split("/")[-1]   # ./models
SAVE_QUANT_DIR = LOCAL_MODEL_DIR.parent / f"{LOCAL_MODEL_DIR.name}-W8A8"

NUM_CALIBRATION_SAMPLES = 512
MAX_SEQUENCE_LENGTH = 2048

# ---------------- 2. 设置镜像（清华） ----------------
# 只对本次运行生效，不污染全局
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

# ---------------- 4. 加载本地模型/分词器 ----------------
from transformers import AutoTokenizer, AutoModelForCausalLM

print(">>> 从本地加载模型 …")
model = AutoModelForCausalLM.from_pretrained(
    LOCAL_MODEL_DIR,
    torch_dtype="auto",
    device_map="auto",
    local_files_only=True
)
tokenizer = AutoTokenizer.from_pretrained(
    LOCAL_MODEL_DIR,
    local_files_only=True,
    use_fast=True
)

# ---------------- 5. 准备校准数据 ----------------
from datasets import load_dataset

print(">>> 加载校准数据集 …")
ds = load_dataset(
    "HuggingFaceH4/ultrachat_200k",
    split=f"train_sft[:{NUM_CALIBRATION_SAMPLES}]"
)
ds = ds.shuffle(seed=42)

def preprocess(example):
    return {"text": tokenizer.apply_chat_template(example["messages"], tokenize=False)}

ds = ds.map(preprocess)

def tokenize(sample):
    return tokenizer(sample["text"],
                     padding=False,
                     max_length=MAX_SEQUENCE_LENGTH,
                     truncation=True,
                     add_special_tokens=False)

ds = ds.map(tokenize, remove_columns=ds.column_names)

# ---------------- 6. 量化 ----------------
from llmcompressor import oneshot
from llmcompressor.modifiers.quantization import GPTQModifier

recipe = GPTQModifier(targets="Linear", scheme="W8A8", ignore=["lm_head"])

print(">>> 开始 GPTQ 量化 …")
oneshot(
    model=model,
    dataset=ds,
    recipe=recipe,
    max_seq_length=MAX_SEQUENCE_LENGTH,
    num_calibration_samples=NUM_CALIBRATION_SAMPLES,
)

# ---------------- 7. 保存量化结果 ----------------
print(">>> 保存量化模型 …")
SAVE_QUANT_DIR.mkdir(parents=True, exist_ok=True)
model.save_pretrained(SAVE_QUANT_DIR, save_compressed=True)
tokenizer.save_pretrained(SAVE_QUANT_DIR)
print(">>> 量化完成，已保存到：", SAVE_QUANT_DIR)
```

# 4、模型评估
*记得先打开梯子再进行评估，否则评估时的数据集无法成功加载*
先运行：
```python
from vllm import LLM
model = LLM("./models/Llama", gpu_memory_utilization=0.95) # 防止爆显存!
```
然后命令行：
```bash
lm_eval --model vllm \
  --model_args pretrained="./models/TinyLlama-1.1B-Chat-v1.0-W8A8",add_bos_token=true \
  --tasks gsm8k \
  --num_fewshot 5 \
  --limit 250 \
  --batch_size 'auto' \
  --output_path eval_out/Llama_out
```

## 如果出现命令行说无法识别lm_eval问题
直接下载
```bash
pip install lm_eval
```
