# 1、准备阶段
## 创建虚拟环境
1. 使用下面的语句在当前文件夹创建虚拟环境文件夹`myenv`
```bash
conda create -p ./myenv python=3.10.0 -y
```
2. 激活bash，虽然已经创建了虚拟环境，但是shell还没有识别
```bash
conda init bash
source ~/.bashrc
```
3. 激活虚拟环境
```bash
conda activate ./myenv
```
4. 下载依赖
```bash
conda install pip
pip install vllm
pip install llmcompressor
```

## 解决网络问题
在AutoDL的帮助文档中找到“学术资源加速”  
命令行输入指令：
```
source /etc/network_turbo
```
**如果要用到huggingface的数据集**，还需要：
```
export HF_ENDPOINT=https://hf-mirror.com
```

# 2、模型下载
本次使用`Llama3.1-401B-Instruct`生成的数据集`nvidia/OpenMathInstruct-2`。  
该数据集是Math和gsm8k数据集合并增强之后，由Llama3生成了思维链CoT，刚好可以满足我们的需求，可将Llama3看作我们的教师（这次真的是很强力的教师，且不用下载到本地就获得了输出）  
那么接下来我们只需要选择学生模型即可，我们选择和Llama3模型结构更接近的Llama3.2-1B

使用魔搭平台一键下载(https://www.modelscope.cn/)
```bash
modelscope download --model LLM-Research/Meta-Llama-3.1-8B-Instruct --local_dir ./models/Llama-3.1-8B-Instruct
```

# 3、数据集下载
```python
from datasets import load_dataset, Dataset
import os
import json

# ===========================
# 配置
# ===========================
custom_cache_dir = "./datasets/OpenMathInstruct"
os.makedirs(custom_cache_dir, exist_ok=True)
N = 10000  # 只取 10000 条

# ===========================
# 1. 使用 streaming 下载
# ===========================
streaming_ds = load_dataset(
    "nvidia/OpenMathInstruct-2",
    split="train",
    cache_dir=custom_cache_dir,
    streaming=True
)

# ===========================
# 2. 取前 N 条样本
# ===========================
streaming_subset = streaming_ds.take(N)

# ===========================
# 3. 转换为 Hugging Face Dataset
# ===========================
# Dataset.from_generator 需要一个生成器函数
def gen():
    for example in streaming_subset:
        yield example

ds_small = Dataset.from_generator(gen)

# ===========================
# 4. 保存到本地
# ===========================
save_path = os.path.join(custom_cache_dir, "train_10k.jsonl")

with open(save_path, "w", encoding="utf-8") as f:
    for example in ds_small:
        f.write(json.dumps(example, ensure_ascii=False) + "\n")

print(f"✅ 已保存 {len(ds_small)} 条样本到 {save_path}")

```

# 4、模型微调

## 浏览模型结构
```python
from transformers import AutoModelForCausalLM
import torch

MODEL_PATH = "./models/Llama-3.1-8B-Instruct"

print("Loading model from:", MODEL_PATH)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    torch_dtype=torch.float16,
    device_map="cpu",    # CPU 加载即可，不会占用 GPU
    trust_remote_code=True
)

print("Model loaded. Listing all module names...\n")

for name, module in model.named_modules():
    print(name)

print("\nTotal modules:", len(list(model.named_modules())))

```

## 进行LoRA微调
```python

```

# 5、评估
```bash
lm_eval --model vllm \
  --model_args pretrained="./models/Qwen2.5-7B-Math",add_bos_token=true \
  --tasks gsm8k,mathqa \
  --num_fewshot 5 \
  --limit 500 \
  --batch_size 'auto' \
  --output_path eval_out/Qwen_out
```
