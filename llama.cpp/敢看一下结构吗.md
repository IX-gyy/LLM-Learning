# llama.cpp的总体结构
```
llama.cpp/
├── include/               # 公共头文件（API 接口）
├── src/                   # 核心实现源码
├── ggml/                  # 张量计算库（核心依赖）
├── common/                # 公共工具库（CLI 示例用）
├── examples/              # 示例程序
├── tests/                 # 测试用例
├── scripts/               # 辅助脚本
└── docs/                  # 文档
```

## src——公共API
- `llama.h`：核心API头文件，定义所有公开函数和数据结构
- `llama.cpp`：实现了`llama.h`，包含模型加载、上下文管理、推理逻辑等多个功能
- `llama-arch.h`：支持的模型架构配置
- `llama-model.h`：模型结构定义和加载
- `llama-context.h`：上下文管理（KV Cache、采样器等）
- `llama-graph.h`：计算图构建和管理
- `llama-kv-cache.h`：KV Cache实现
- `llama-sampling.h`：采样策略（温度、top_p等）
- `llama-vocab.h`：词表/分词器实现
- `llama-adapter.h`：LoRA支持

### llama.h中的重点
```cpp
// 三大核心对象
struct llama_model;      // 模型对象（只读，可共享）
struct llama_context;    // 上下文（状态ful，每个会话独立）
struct llama_vocab;      // 词表对象
```
典型使用流程：
```cpp
llama_model_load();      // 1. 加载模型
llama_new_context();     // 2. 创建上下文
llama_decode();          // 3. 前向推理（可重复调用）
llama_sample_token();    // 4. 采样生成
llama_free();            // 5. 释放资源
```

### llama.cpp
#### 1.参数自适应内存管理
`llama_params_fit`，用于自动调整模型参数以自适应可用内存  
- 获取设备内存数据，计算当前参数下各设备的内存使用情况
- 调整上下文大小，内存不足，减少`n_ctx`
- 分层加载（Dense/MoE）：从后向前填充层，支持部分层溢出到CPU内存
- MoE优化：将MoE模型的稀疏权重放入系统内存

#### 2.模型加载与初始化
```cpp
// 对外接口
struct llama_model * llama_model_load_from_file(
    const char * path_model,
    struct llama_model_params params);

// 内部实现
static struct llama_model * llama_model_load_from_file_impl(...) {
    // 1. 初始化时间管理
    // 2. 检查后端是否加载
    // 3. 设置进度回调
    // 4. 创建设备列表（GPU/RPC/集成GPU）
    // 5. 处理单 GPU 模式
    // 6. 调用 llama_model_load 实际加载
}
```
#### 3.后端设备管理（CPU/GPU）

#### 4.聊天模板

#### 5.系统信息查询

## ggml——张量计算引擎
- `ggml.c`：张量操作实现，定义了`ggml_tensor`、`ggml_cgraph`
- `ggml-backend.h`：后端接口，统一GPU、CPU调度
- `ggml-alloc.h`：内存分配
- `ggml-cuda`：CUDA后端实现
- `ggml-cpu`：CPU后端

核心设计模式：
```cpp
// 1. 构建计算图（Define）
struct ggml_cgraph * gf = ggml_new_graph(ctx);
struct ggml_tensor * x = ggml_new_tensor_2d(ctx, GGML_TYPE_F32, 640, 10);
struct ggml_tensor * w = ggml_new_tensor_2d(ctx, GGML_TYPE_F32, 640, 4096);
struct ggml_tensor * r = ggml_mul_mat(ctx, w, x);  // 矩阵乘法
ggml_build_forward_expand(gf, r);

// 2. 执行计算（Run）
ggml_graph_compute_with_ctx(ctx, gf, n_threads);
```

## example——示例资料
simple.cpp展示了一个最简单的推理示例：
```cpp
// 1. 初始化后端
ggml_backend_load_all();

// 2. 加载模型
llama_model_params model_params = llama_model_default_params();
llama_model * model = llama_load_model_from_file(model_path, model_params);

// 3. 创建上下文
llama_context_params ctx_params = llama_context_default_params();
ctx_params.n_ctx = 2048;  // 上下文长度
llama_context * ctx = llama_new_context_with_model(model, ctx_params);

// 4. 分词
std::vector<llama_token> tokens = tokenize(vocab, prompt, true);

// 5. 推理循环
while (true) {
    llama_decode(ctx, batch);           // 前向计算
    llama_token new_token = sample(ctx); // 采样
    if (is_eog(new_token)) break;        // 结束符检查
    tokens.push_back(new_token);
    batch = llama_batch_init(1, 0, 1);
    batch.token[0] = new_token;
}

// 6. 清理
llama_free(ctx);
llama_free_model(model);
```

















