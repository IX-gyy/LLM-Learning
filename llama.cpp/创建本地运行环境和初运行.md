# 创建运行环境
## 推荐使用wsl运行的Ubuntu
✅ 强烈推荐：本地电脑的 Linux（虚拟机 or WSL2）  
优先级建议：  
Windows + WSL2（Ubuntu）⭐⭐⭐⭐⭐（最推荐）  
本地 Linux 虚拟机（VMware / VirtualBox）⭐⭐⭐⭐  
云端服务器（仅当你已有 GPU 或远程开发经验）⭐⭐  

❌ 不推荐  
Windows 原生环境（MSVC / MinGW）  
能编译  
但：调试痛苦、路径问题多、生态不一致  
对“理解源码”这个目标是负收益  
二、为什么不直接 Windows 原生？  

你现在的目标是 读代码 + 调试 + 修改，不是“能跑就行”。  

Windows 原生的问题：
- CMake + MSVC 编译参数复杂
- gdb / lldb 支持不友好
- 后面 CUDA、Nsight 会更痛苦

llama.cpp 主要开发者是 Linux / macOS 环境  

## 安装环境
安装wsl
```powershell
wsl --install
```

安装Ubuntu（推荐 22.04）  
可能需要魔法，加上web-download绕过Microsoft Store下载更快：
```
wsl --install -d Ubuntu-22.04 --web-download
```
初次安装Ubuntu需要输入用户名和密码，自己记住  
进入Ubuntu后：
```
sudo apt update
sudo apt install -y \
  build-essential \
  cmake \
  git \
  python3 \
  python3-pip \
  gdb
```
环境就初步搭建成功了！

## 克隆仓库和初运行
克隆llama.cpp：
```
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp
```
用最简单的方式进行编译：
```
mkdir build
cd build
cmake ..
cmake --build . -j
```
可能是已经改版的原因，~~一些建议是运行以下命令，但是经过验证这是不成功的~~：
```
# 不要用
./llama-cli --help
# 或旧版本可能是
./main --help
```
实际上运行`ls -lh`后，build文件夹内容如下：
> total 396K  
-rw-r--r--  1 CMakeCache.txt  
drwxr-xr-x 33 CMakeFiles  
-rw-r--r--  1 CTestTestfile.cmake  
-rw-r--r--  1 DartConfiguration.tcl  
-rw-r--r--  1 Makefile  
drwxr-xr-x  3 Testing  
drwxr-xr-x  2 bin  
-rw-r--r--  1 cmake_install.cmake  
drwxr-xr-x  3 common  
-rw-r--r--  1 compile_commands.json  
drwxr-xr-x 24 examples  
drwxr-xr-x  4 ggml  
-rw-r--r--  1 license.cpp  
-rw-r--r--  1 llama-config.cmake  
-rw-r--r--  1 llama-version.cmake  
-rw-r--r--  1 llama.pc  
drwxr-xr-x  4 pocs  
drwxr-xr-x  3 src  
drwxr-xr-x  3 tests  
drwxr-xr-x 18 tools  
drwxr-xr-x  3 vendor  

实际上都在`bin`文件夹中：  
- `llama-cli`（主程序，用于命令行推理）
- `llama-server`（Web服务程序）
- `llama-quantize`（模型量化工具）
- 其他工具...

看见有这些文件就证明我们的初步编译已经成功了！运行`./bin/llama-cli --help`，可以查看帮助  

## 利用模型进行简单推理
我们使用一个模型进行快速的推理测试
```
# 回到项目根目录
cd ..

# 创建模型目录
mkdir -p models

# 下载测试模型（TinyLlama，1.1GB，速度快）
wget https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf -P models/

# 运行推理
./build/bin/llama-cli \
  -m models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf \
  -p "你好，请用中文介绍自己" \
  -n 256 \
  -e
```
